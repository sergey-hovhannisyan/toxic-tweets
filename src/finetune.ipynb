{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgTOocy70xHB"
      },
      "source": [
        "## ___Toxic Tweets Fine-Tuned Pretrained Transformer with Multi Head Classifier___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGpWnNct0xHC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6JovL-hI8JKQ"
      },
      "outputs": [],
      "source": [
        "# If GPU is available, use it, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBhZv3Sc0xHD"
      },
      "outputs": [],
      "source": [
        "# Getting training dataset\n",
        "df = pd.read_csv(\"../data/raw/train.csv\")\n",
        "# Comments as list of strings for training texts\n",
        "texts = df[\"comment_text\"].tolist()\n",
        "# Labels extracted from dataframe as list of lists\n",
        "labels = df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].values.tolist()\n",
        "# Training set split into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q0OqpFmZ0xHD"
      },
      "outputs": [],
      "source": [
        "# Tokenizing training and validation sets\n",
        "class ToxicTweetsDataset(Dataset):\n",
        "    # Initialize the class variables\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "    # Returns the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "    # Returns a dictionary of the tokenized text, attention mask, and labels\n",
        "    def __getitem__(self, index):\n",
        "        input_ids = self.encodings['input_ids'][index]\n",
        "        attention_mask = self.encodings['attention_mask'][index]\n",
        "        labels = torch.tensor(self.labels[index], dtype=torch.float32)\n",
        "        return {'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'labels': labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRQlGBz3Dwbs"
      },
      "outputs": [],
      "source": [
        "# Choosing base pretrained model\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEtsC8-Y0xHE"
      },
      "outputs": [],
      "source": [
        "# Tokenizing and encoding training and validation sets\n",
        "train_encodings = tokenizer.batch_encode_plus(train_texts, truncation=True, padding=True, return_tensors='pt')\n",
        "val_encodings = tokenizer.batch_encode_plus(val_texts, truncation=True, padding=True, return_tensors='pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjQ_ZuJxLnb8"
      },
      "outputs": [],
      "source": [
        "# # Saving encoded and tokenized data to files\n",
        "# torch.save(train_encodings, '../data/tokenized_encodings/train_encodings.pt')\n",
        "# torch.save(val_encodings, '../data/tokenized_encodings/val_encodings.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqmHgIXwNMJQ"
      },
      "outputs": [],
      "source": [
        "# # Loading encoded and tokenized data from files\n",
        "# train_encodings = torch.load('../data/tokenized_encodings/train_encodings.pt').to(device)\n",
        "# val_encodings = torch.load('../data/tokenized_encodings/val_encodings.pt').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmEatZLqDt6p"
      },
      "outputs": [],
      "source": [
        "# Creating datasets for training and validation\n",
        "train_dataset = ToxicTweetsDataset(train_encodings, train_labels)\n",
        "val_dataset = ToxicTweetsDataset(val_encodings, val_labels)\n",
        "\n",
        "# Creating model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, problem_type=\"multi_label_classification\", num_labels=6)\n",
        "model.to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Setup & Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CIjI_Q75haw"
      },
      "outputs": [],
      "source": [
        "# Setting training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"../models/fine_tuned\",\n",
        "    num_train_epochs=2, \n",
        "    per_device_train_batch_size=16, \n",
        "    per_device_eval_batch_size=32, \n",
        "    warmup_steps=500,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        ")\n",
        "\n",
        "# Creating trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        ")\n",
        "\n",
        "# Training model\n",
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing The Fine-Tuned Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading test dataset\n",
        "test_size = 10000\n",
        "test_df = pd.read_csv(\"../data/raw/test.csv\")\n",
        "test_label_df = pd.read_csv(\"../data/raw/test_labels.csv\")\n",
        "\n",
        "# Comments as list of strings for testing texts\n",
        "test_texts = test_df[\"comment_text\"].tolist()[:test_size]\n",
        "# Labels extracted from dataframe as list of lists\n",
        "test_labels = test_label_df[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].values.tolist()[:test_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading model and tokenizer\n",
        "model = DistilBertForSequenceClassification.from_pretrained(\"sergey-hovhannisyan/fine-tuned-toxic-tweets\")\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "model.eval()\n",
        "\n",
        "# Tokenizing and encoding test set\n",
        "test_encodings = tokenizer.batch_encode_plus(test_texts, truncation=True, padding=True, return_tensors='pt')\n",
        "\n",
        "# Creating datasets for testing set\n",
        "test_dataset = ToxicTweetsDataset(test_encodings, test_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code sets a batch size for evaluation, creates a DataLoader object for the test dataset, and then iterates over the batches in the test set. For each batch, it moves the data to the GPU (if available), uses the trained model to make predictions, and then appends the batch predictions and labels to two lists. Finally, it combines all batch predictions and labels into one array each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch size for evaluation\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Create lists for preds and labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "# Loop over batches\n",
        "for batch in test_loader:\n",
        "    # move batch to GPU if available\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        # make predictions\n",
        "        outputs = model(**batch)\n",
        "        logits = outputs.logits\n",
        "        preds = torch.sigmoid(logits)\n",
        "        preds = (preds > 0.5).int()\n",
        "    # append predictions and labels to lists\n",
        "    all_preds.append(preds.cpu().numpy())\n",
        "    all_labels.append(batch['labels'].cpu().numpy())\n",
        "\n",
        "# Combine all predictions and labels\n",
        "all_preds = np.concatenate(all_preds, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the current scenario, we are only evaluating the \"toxic\" label column since the test label dataset assigns a value of -1 to all labels if any form of toxicity is detected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Getting only toxic label from predictions & labels\n",
        "all_preds_toxic = -1*all_preds[:,0]\n",
        "all_labels_toxic = all_labels[:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculating metrics\n",
        "def compute_metrics(labels, predictions):\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision = precision_score(labels, predictions, average='weighted')\n",
        "    recall = recall_score(labels, predictions, average='weighted')\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1 }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate evaluation metrics\n",
        "metrics = compute_metrics(all_labels_toxic, all_preds_toxic)\n",
        "\n",
        "# print evaluation metrics\n",
        "print('Recall: ', round(metrics['recall'],4))\n",
        "print('Precision: ', round(metrics['precision'],4))\n",
        "print('Accuracy: ', round(metrics['accuracy'],4))\n",
        "print('F1: ', round(metrics['f1'],4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
